@article{Herlocker2004,
abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
archivePrefix = {arXiv},
arxivId = {50},
author = {Herlocker, Jonathan L. and Konstan, Joseph a. and Terveen, Loren G. and Riedl, John T.},
doi = {10.1145/963770.963772},
eprint = {50},
isbn = {1011459637},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
mendeley-groups = {Babel},
number = {1},
pages = {5--53},
title = {{Evaluating collaborative filtering recommender systems}},
volume = {22},
year = {2004}
}
@article{Beel2013,
address = {New York, New York, USA},
author = {Beel, Joeran and Genzmehr, Marcel and Langer, Stefan and N\"{u}rnberger, Andreas and Gipp, Bela},
doi = {10.1145/2532508.2532511},
file = {:Users/iwsmith/Library/Application Support/Mendeley Desktop/Downloaded/Beel et al. - 2013 - A comparative analysis of offline and online evaluations and discussion of research paper recommender system evalua.pdf:pdf},
isbn = {9781450324656},
journal = {Proceedings of the International Workshop on Reproducibility and Replication in Recommender Systems Evaluation - RepSys '13},
keywords = {click-through rate,comparative study,evaluation,offline,online evaluation,research paper recommender systems},
mendeley-groups = {JWest - Lit Review},
pages = {7--14},
publisher = {ACM Press},
title = {{A comparative analysis of offline and online evaluations and discussion of research paper recommender system evaluation}},
url = {http://dl.acm.org/citation.cfm?doid=2532508.2532511},
year = {2013}
}
@article{Beel2013survey,
abstract = {Fala sobre a qualidade da avaliacao dos artigos sobre recomendacao de papers.},
annote = {Abstract},
author = {Beel, Joeran and Langer, Stefan and Genzmehr, Marcel and Gipp, Bela and Breitinger, Corinna and N\"{u}rnberger, Andreas},
doi = {10.1145/2532508.2532512},
file = {:Users/iwsmith/Dropbox/Documents/Papers/research\_paper\_recommender\_system\_evaluation--a\_quantitative\_literature\_survey.pdf:pdf},
isbn = {9781450324656},
journal = {RepSys},
keywords = {comparative,evaluation,recommender systems,research paper recommender systems,study,survey},
mendeley-groups = {JWest - Lit Review},
number = {April},
pages = {15--22},
title = {{Research Paper Recommender System Evaluation: A Quantitative Literature Survey}},
url = {http://dl.acm.org/citation.cfm?id=2532508.2532512},
year = {2013}
}
@article{Sinha2015,
author = {Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-june Paul},
file = {:Users/iwsmith/Dropbox/Documents/Papers/p243.pdf:pdf},
isbn = {9781450334730},
mendeley-groups = {JWest - Lit Review},
pages = {243--246},
title = {{An Overview of Microsoft Academic Service ( MAS ) and Applications}},
year = {2015}
}

