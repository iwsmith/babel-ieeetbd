
%% bare_jrnl_compsoc.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************




\documentclass[10pt,journal,compsoc]{IEEEtran}


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{./img/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Babel: A platform for research in scholarly article recommendation}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Ian~Wesley-Smith,
		Jevin~West%
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem J. West and I. Wesley-Smith are with
the Information School, University of Washington, Seattle, WA, 98195-1800.\protect\\
E-mail: \{jwest, iwsmith\}@uw.edu}%
\thanks{Manuscript received July 31, 2015}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{IEEE Transactions on Big Data}%
{Wesley-Smith \MakeLowercase{\textit{et al.}}: A platform for research in scholarly article recommendation}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2014 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Researchers developing new recommendation algorithms for scholarly articles face two substantial hurdles: datasets are very difficult to acquire and there is no standardized mechanism for evaluation of these algorithms.
We are proposing the creation of a Babel, a research platform to help alleviate both of these issues and allow researchers to focus on what they do best -- developing novel recommenders.
\end{abstract}}

% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.



\IEEEPARstart Researchers working to develop scholarly article recommenders face two major obstacles in their research.
First is the difficulty of finding datasets.
Since much of scholarly communication is behind publisher paywalls it is difficult to collect data.
Few publishers have made any of their datasets available, and those that are available are legally encumbered, meaning third parties cannot replicate experiments with them.
Some have chosen to crawl publishers to create their own datasets, but this risks legal action over redistribution of copyrighted material, also limiting reproducibility. %TODO: Is this true or did I just make it up? I am pretty sure I made it up and that crawling is legally OK, just a huge PITA.
Some of the more forward thinking publishers have released open datasets to help spur interest in this domain, but these datasets tend to be small in size relative to the overall corpus, while others are of poor quality.
This leaves researchers in a difficult situation: invest incredible effort in getting these datasets from publishers with the knowledge that your experiments can't be replicated, crawl the datasets and risk legal action, or use the small publicly available data sets which may not be suitable for recommendation purposes.

The second issue researchers face is determining the efficacy of their recommenders.
Although techniques exist for evaluating recommenders in other domain with explicitly rated items (such as movies or products) there is no agreement on how recommenders for datasets without explicit ratings should be evaluated.
This has led to a proliferation of ad-hoc techniques and a lack of rigor that has stifled progress in the field, with an inability to determine which avenues of investigation are fruitful.
In a comprehensive survey of over 170 articles on research paper recommenders Beel et al. concluded ``... that it is currently not possible to determine which recommendation approaches for academic literature are the most promising. However, there is little value in the existence of more than 80 approaches if the best performing approaches are unknown.''\cite{Beel2013survey}

We believe a unified platform for researchers can help solve both of these issues, and have begun work on such a project.
We call this project Babel after the short story by Jorge Luis Borges ``The Library of Babel''. 
The story describes an infinite library filled with all possible books. 
Most of the books contain gibberish, but amongst the gibberish is also a description of the future, the past, and the present.
We felt it was a fitting metaphor for navigating scholarly articles: somewhere in all the noise is the paper you need.
Babel provides scholarly article recommendations as a service, freely available to anyone in the world.
It is our hope that such a service will spur research into scholarly article recommenders and paper management systems, making the pursuit of science more efficient for everyone.

\section{Goals and Scope}
We have the following goals for Babel:
\begin{enumerate}
	\item Allow for the evaluation and comparison of recommender algorithms for scholarly literature
	\item Decrease the difficulty of developing recommender algorithms for scholarly literature
	\item Increase the quality of recommendations available for scholarly literature
	\item Provide enterprise grade reliability and performance on the platform so publishers feel comfortable using it in production.
\end{enumerate}

There are a few items we feel are out of scope for the initial release of Babel, but likely would be added in the future:
\begin{itemize}
	\item Act as a search engine for scholarly literature
	\item Support collaborative filtering recommenders
\end{itemize}

Finally there are several explicit non-goals; these are things we don't intend for Babel to do:
\begin{itemize}
	\item Store full-text copies of scholarly literature
	\item To be a paper management system %TODO: I am sure we have more non-goals..
\end{itemize}

\begin{table}
\centering
  \begin{tabular}{@{} cccc @{}}
    \hline
    Dataset & Papers & Citations & Recommendations \\ 
    \hline
	aminer* & 2,092,356 & 8,024,869 & 22,112,496 \\ 
    JSTOR & 1,787,351 & 8,227,537 & 14,813,224 \\ 
    PLOS & 1,599,712 & 3,232,766 & 8,647,037 \\ 
    PubMed* & 5,538,322 & 16,004,596 & 34,026,854 \\ 
    arXiv & 626,441 & 781,108 & 5,624,262 \\ 
    DBLP* & 781,108 & 4,191,677 & 2,163,313 \\ 
    MAS* & 27,352,532 & N/A & 245,796,494 \\     
    \hline
	Total & 39,777,822 & 40,462,553 & 333,183,680 \\
  \end{tabular}
\caption{Datasets currently available on Babel. A * denotes an open access dataset}
\label{table:datasets}
\end{table}

\begin{figure*}
\centering
\includegraphics[width=6.5in]{babel_site}
\caption{Screenshot of the Babel demonstration website. This single-app page is powered by Babel.}
\label{fig:demo_site}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=6.5in]{chrome_plugin}
\caption{Screenshot of our Google Chrome plugin which provides recommendations directly on the Google Scholar site. This plugin is powered by Bable.}
\label{fig:chrome_plugin}
\end{figure*}

\section{Audience}

%TODO: Better explain the value proposition to both groups
This service is built primarily for two groups of people: consumers of scholarly article recommendations and producers of those recommendations.
The first group is likely publishers, specifically smaller ones who may not have the time or budget to generate high quality recommendations for their users. 
However, since these recommendations are freely available there are many other possible consumers.
For example, a group working on developing better paper management software might want to integrate recommendations to help users find related material.
We have developed several tools based on this platform: a demonstration website at \url{recommends.eigenfactor.org}, a Google Chrome plugin that will provide recommendations as you search Google Scholar, and a collaborative effort with JSTOR in their experimental Sustainability collection\footnote{\url{http://labs.jstor.org/sustainability/}} %TODO: include chrome plugin link
We invite anyone to develop tooling on top of this platform.

The second group are researchers developing novel recommenders for scholarly articles.
Researchers will develop against the open datasets in the Babel collection, all of which are provided in a standardized format.
Once they have successfully integrated against the open datasets they will send us their recommenders and we will run them against the closed datasets and begin providing those recommendations to consumers.
The next day they will be able to compare the performance of their algorithm to other recommenders on live users.
We hope that this rapid evaluation will allow researchers to focus on improving the quality of their algorithms, not the details of acquiring datasets and tracking user behavior. %TODO: Maybe clarify that rapid iteration is a good thing, draw it out more

\section{Licensing}
Though much of the data we supply is proprietary, the entire source code for the platform will be open source and available on GitHub.
Our goal is to allow other groups to easily setup their own version of Babel, providing API compatibility while keeping their algorithms or data in-house.
Authors of recommender algorithms maintain full IP rights to their recommenders, though recommendations on the Babel platform hosted by us will be distributed freely and unencumbered. 
Recommender authors can, at any time, ask us to remove their recommender and we will comply in a timely fashion.

\section{Evaluation}%TODO: This first paragraph needs some love
One of the primary drivers for creating this platform is to facilitate the evaluation of various scholarly article recommenders.
Generally there are two approaches for evaluation of recommender systems: offline and online.
When a ground truth is available, such as the user ratings included in the MovieLens\footnote{The movie lens data set is freely available and contains 20 million ratings of 27,000 movies by 138,000 users. It is available at \url{http://http://grouplens.org/datasets/movielens/}} dataset, offline evaluation is straightforward.
You train the recommender on some portion of the data set, then have it make predictions on data that was not used for training.
By comparing the recommender's prediction against a users actual rating you can evaluate how accurate (or precise) a recommender is.
Unfortunately scholarly articles are not rated by their users.
Even if we were to try and collect ratings the scarcity of  user-item interactions would leave us with little training or cross-validation data.
The MovieLens dataset, by contrast, contains 20 million ratings for 27,000 movies; nearly three orders of magnitude more user-item interactions than there are items.

When a ground truth isn't available qualitative evaluations are often used.
A common approach is to ask experts in a given field to rate the quality of recommendations generated by an algorithm.
Although this method can provide extremely useful information to guide development of these algorithms, it is inadequate for comparing different algorithms.
One of the main reasons is its lack of scalability: finding experts is difficult and asking them to rate recommendations is a time consuming task for them.
If you want to generate even a relatively small number of recommendations, say ~.01\% of all scholarly literature (Sinha et al. reported 83 million articles crawled\cite{Sinha2015} in their latest dataset), that is 8,300 recommendations, or 83,000 papers (assuming 10 papers per-recommendation).
Clearly the amount of data involved is too large for one expert, or even a hundred experts.
There are also questions of bias and coverage; everyone has certain preferences, and in some fields it may be difficult to find an expert.
Finally the methodology isn't reproducible: I cannot take your offline dataset, run my recommender and compare our results without using an expert of my own (and ideally the same expert).
Even if one could, how do you deal with conflicting expert rankings? %TODO: Improve our anti-expert bias

This idea that scholarly article recommenders are not amenable to offline evaluation has been studied by Beel et al., concluding ''our research showed that offline evaluations could not reliably predict an algorithm’s click-through performance in practice.''\cite{Beel2013}
In fact, they found that offline evaluation performed very poorly for citation based methods: ''the offline evaluation predicted a disappointing result of 0.96\%. In practice, the citation-based approach had a CTR [click-through rate] of 8.27\%''\cite{Beel2013}
Given this strong condemnation of offline evaluation it is surprising that a survey of scholarly recommenders\cite{Beel2013survey} found only 5 of the 89 approaches were evaluated with online approaches.
The most likely reason for this is the difficulty of gathering users for online evaluation and the overhead of tracking their responses.

With all of this in mind we propose online evaluation using long-term, observational field studies focused on two outcomes: did a user view a paper's abstract and did a user download a paper.
We chose these metrics because they are user centric and they make good optimization goals. 
Ultimately we want users to spend less time sifting through irrelevant papers and more time reading the ones that matter.
We define the following metrics which Babel will generate on a per-algorithm basis at regular intervals:
\begin{itemize}
\item Click-through rate (CTR): clicks/impressions * 100\%
\item Download rate (DR): downloads/impressions * 100\%
\item Transition rate (TR): downloads/clicks * 100\%
\end{itemize}
%TODO: Maybe add a coverage metric?
Impressions is defined as the number of times a recommendation generated by the algorithm was shown.
Note that all of these metrics are relative to other recommenders, they don't provide an absolute measure of performance.
This is why these metrics will all be freely available to be the public for all algorithms, with full histories and various time windows available for analysis and comparison.
Babel's feedback mechanism collects additional metadata (most optional) which can be used to group or filter data, including: consumer (i.e. publisher/platform), session identifier, database (e.g. PLOS, arXiv, MAS), pseudonymous user identifier and time.

\begin{figure*}
\centering
\includegraphics[width=6.5in]{architecture}
\caption{Babel's Architecture. 1) Public endpoint, exposing a REST API that provides scholarly article recommendations. 2) Recommendation generation and dataset ingestion.}
\label{fig:arch}
\end{figure*}

\section{Architecture}
Babel is designed to provide recommendations as a service via REST APIs.
Although our implementation is hosted on AWS, it could be run on any cloud provider with some modifications or completely in house if that was desired.
Our design goals centered around providing low-latency recommendations suitable for client consumption from a web browser, while scaling horizontally to support a large number of concurrent users. %TODO: Would be nice to have an actual number

\subsection{Public Endpoint}
The public endpoint is what consumers will interact with, and is represented in figure~\ref{fig:arch} item (1).
This endpoint exposes a REST API, which, given a paper identifier and a publisher, will generate a list of recommendations with various recommendations algorithms, returned in JSON format. 
Endpoints will pull recommendations for the recommendation cache, and emit analytics events to allow for evaluation of various recommenders performance. 
Detailed documentation of the current API exposed by the endpoint is available at \url{http://babel.eigenfactor.org/api.html}%TODO: Should we note the search API that is available?

\subsection{Recommendation Generation}
Recommendation generation, represented in figure~\ref{fig:arch} by item 2, occurs whenever a dataset is updated.
Once a publisher has uploaded a new dataset to Babel, metadata is extracted and stored in the metadata database.
Simultaneously the publisher's raw data is archived and then transformed into a normalized format suitable for ingestion by recommender algorithms.
Once normalization has occurred all available recommenders are run in parallel.
Their output is then pushed to the recommendation cache, which is currently implemented in DynamoDB. 
At this point queries against the frontend will return new recommendations. 

\subsection{Feedback}%TODO: Feels a bit light here, probably because a lot of this is theoretical :-/
Given that evaluation of recommenders is one of the primary goals of this platform we need to have a mechanism to submit feedback about recommendations.
Currently, we have an API in place for submitting feedback, allowing recommendation consumers (e.g. publishers) to signal what action was taken by a user.
There are two actions we support, click, which denotes a user clicking on a recommendation and being directed to page with more information about the article, and download, which denotes a user actually downloading an article.
The API also allows consumers to add annotations, such as a session identifier, so a user's actions through an entire session can be analyzed.
These annotations must be anonymous to Babel; we should not ever be able to identify an individual user.
As new requirements are added we will expand the capabilities of the analytics system to record data.
All feedback is aggregated on a daily basis and new evaluation metrics published.

\section{Long Term Plans}
Although Babel is currently a research project if it works as intended we foresee a long life for this platform.
We intend to run Babel as a research and development project for the first few years.
Once the platform has shown itself capable of production workloads and if utilization is sufficient it will need to be transitioned over to a more permanent entity for governance and development.
There are several examples of non-profit membership fee based entities, such as CrossRef, ORCID and DOI that provide examples of how common good projects like this can exist. 
It is even possible that this project could be transferred to one of these entities for continuing development. 
Finally, since the platform code is open-source, in the worst case scenario another group could fork the code base and start running a replacement platform.

\section{Current Status}
Currently we've built the the public endpoint, allowing for recommendations to be delivered and feedback on user actions to be submitted and recorded.
Babel currently delivered EigenFactor Recommends Expert and Classic recommendations\cite{West2015}, with co-citation being added shortly.
Our current goal is to on-board several publishers of various sizes to validate our endpoint design, specifically around latency, scalability and analytics.
Once we have done this we will begin soliciting regular updates from publishers to validate the recommendation generation phase.
Finally, we will open the system to researchers who wish to implement new recommendation algorithms.
You can try out the platform at, read API documentation or learn more about the project at \url{http://babel.eigenfactor.org}.

% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

This work was supported the Metaknowledge Network funded by the John Templeton Foundation. 
We would like to thank Kyle Estlick for his development of the Google Chrome plugin and Nick Thorpe for feedback on early drafts of this paper.


% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{./platform.bib}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

% that's all folks
\end{document}
